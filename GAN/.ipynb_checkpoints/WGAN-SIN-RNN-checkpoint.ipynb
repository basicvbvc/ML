{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from torch.nn.init import xavier_normal\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from pycrayon import CrayonClient\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_hid = 64\n",
    "D_layers = 1\n",
    "\n",
    "G_hid = 32 \n",
    "G_layers= 1\n",
    "\n",
    "mb_size = 64\n",
    "seq_len = 20\n",
    "z_dim = 10\n",
    "c_dim = 10\n",
    "n_classes = 2\n",
    "\n",
    "\n",
    "def g_noise():\n",
    "    return (torch.randn(seq_len, mb_size, z_dim))\n",
    "\n",
    "def pretrain_noise():\n",
    "    noise = g_noise()\n",
    "    seqs, _ = real_sample()\n",
    "    inp = torch.cat([seqs[:,:,:], noise], 2)\n",
    "    return inp, seqs\n",
    "    \n",
    "def real_sample():\n",
    "    shift_choices = [-1, 2]\n",
    "    shift = np.random.choice(shift_choices, mb_size)\n",
    "    freq = np.ones(mb_size) * 3\n",
    "    support = np.linspace(-1, 1, seq_len)\n",
    "    seqs = np.array([np.sin((support + shift[i])*freq[i]) \n",
    "                   for i in range(mb_size)])\n",
    "    seqs = seqs.T.reshape(seq_len, mb_size, 1)\n",
    "    seqs += np.random.normal(size=seqs.shape) * 1e-1\n",
    "    return Tensor(seqs), 3\n",
    "\n",
    "u, s = real_sample()\n",
    "u = u.numpy()\n",
    "plt.plot(u[:,:,0])\n",
    "plt.show()\n",
    "\n",
    "v = g_noise().numpy()\n",
    "plt.plot(v[0,0,:])\n",
    "plt.show()\n",
    "\n",
    "w = pretrain_noise().numpy()\n",
    "plt.plot(w[:,0,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    def __init__(self, exp_name):\n",
    "        self.cc = CrayonClient(hostname='localhost')\n",
    "        names = self.cc.get_experiment_names()\n",
    "        self.cc.remove_experiment(exp_name)\n",
    "        self.exp = self.cc.create_experiment(exp_name)\n",
    "        \n",
    "    def log_scalar(self, key, val):\n",
    "        self.exp.add_scalar_value(key, val)\n",
    "        \n",
    "    def log_hist(self, key, val):\n",
    "        self.exp.add_histogram_value(key, val, tobuild=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=z_dim+1, \n",
    "            hidden_size=G_hid,\n",
    "            num_layers=G_layers,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        self.decode1 = nn.Linear(G_hid, 1)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        hidden = self.init_hidden()\n",
    "        zs = list(torch.split(z, 1, dim=0))\n",
    "        output = Variable(torch.zeros(1,mb_size,1)).cuda()\n",
    "        outputs = []\n",
    "        for _ in range(seq_len):\n",
    "            zz = torch.cat([output, zs.pop()], 2)\n",
    "            output, hidden = self.gru(zz)\n",
    "            output = F.tanh(self.decode1(output))\n",
    "            outputs.append(output)\n",
    "            \n",
    "        outputs = torch.cat(outputs, 0)\n",
    "        return outputs\n",
    "\n",
    "    def forward_pretrain(self, z):\n",
    "        hidden = self.init_hidden()\n",
    "        output, _ = self.gru(z, hidden)\n",
    "        output = F.tanh(self.decode1(output))\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(\n",
    "            torch.zeros(G_layers, mb_size, G_hid)\n",
    "        ).cuda()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters(): \n",
    "            if param.dim() >= 2:\n",
    "                xavier_normal(param)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1, \n",
    "            hidden_size=D_hid,\n",
    "            num_layers=D_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=0.0,\n",
    "        )\n",
    "        self.decode = nn.Linear(D_hid*2, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        hidden = self.init_hidden()\n",
    "        output, _ = self.gru(X, hidden)\n",
    "        output = self.decode(output)\n",
    "        return output[-1, :, :]\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(\n",
    "            torch.zeros(D_layers*2, mb_size, D_hid)\n",
    "        ).cuda()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters(): \n",
    "            if param.dim() >= 2:\n",
    "                xavier_normal(param)\n",
    "    \n",
    "\n",
    "    \n",
    "G = Generator()\n",
    "G.init_weights()\n",
    "D = Discriminator()\n",
    "D.init_weights()\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "\n",
    "ones_label = Variable(torch.ones(mb_size, 1)).cuda()\n",
    "zeros_label = Variable(torch.zeros(mb_size, 1)).cuda()\n",
    "\n",
    "G_solver = optim.SGD(G.parameters(), lr=1e-3)\n",
    "D_solver = optim.SGD(D.parameters(), lr=1e-3)\n",
    "#G_solver = optim.RMSprop(G.parameters(), lr=1e-3)\n",
    "#D_solver = optim.RMSprop(D.parameters(), lr=1e-3)\n",
    "\n",
    "def reset_grad():\n",
    "    D.zero_grad()\n",
    "    G.zero_grad() \n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_seqs(d, i=None, title=None, ds=10):\n",
    "    d = d.data.cpu().numpy()\n",
    "    plt.plot(d[:,::ds,0])\n",
    "    if title:\n",
    "        plt.title('Loss {}:'.format(title))\n",
    "    plt.savefig('pics/train_epoch{}.pdf'.format(i))\n",
    "    plt.plot(d[:,::ds,0])\n",
    "    if title:\n",
    "        plt.title('Loss {}:'.format(title))\n",
    "    plt.show()\n",
    "\n",
    "def show_seqs_aux(d, i=None, title=None, ds=10):\n",
    "    d = d.data.cpu().numpy()\n",
    "    plt.plot(d[:,::ds,0])\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.savefig('{}.pdf'.format(i))\n",
    "    plt.plot(d[:,::ds,0])\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def show_seqs_nosave(d, i=None, title=None, ds=10):\n",
    "    d = d.data.cpu().numpy()\n",
    "    plt.plot(d.T[:,::ds])\n",
    "    plt.show()   \n",
    "    \n",
    "def freeze(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def show_grad(model, logger, hist_name):\n",
    "    grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        grad = param.grad.data.cpu().numpy().flatten()\n",
    "        grads += grad.tolist()\n",
    "    logger.log_hist(hist_name, grads)\n",
    "    \n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_bar = tqdm_notebook(range(1000))\n",
    "G_losses = []\n",
    "for j, it in enumerate(e_bar):\n",
    "    G_loss_avg = 0\n",
    "    n_batches = 100\n",
    "    b_bar = tqdm_notebook(range(n_batches), leave=False)\n",
    "    \n",
    "    for i in b_bar:\n",
    "        z, target = pretrain_noise()\n",
    "        z = Variable(z).cuda()\n",
    "        target = Variable(target).cuda()\n",
    "                     \n",
    "        output = G.forward_pretrain(z)\n",
    "        \n",
    "        pretrain_loss = F.mse_loss(output, target)\n",
    "        pretrain_loss.backward()\n",
    "        G_solver.step()\n",
    "        pretrain_loss = pretrain_loss.data[0]\n",
    "        \n",
    "        b_bar.set_postfix(loss=pretrain_loss)\n",
    "        G_loss_avg += pretrain_loss\n",
    "        \n",
    "    e_bar.set_postfix(loss=G_loss_avg/n_batches)\n",
    "    G_loss_avg = 0\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_bar = tqdm_notebook(range(1000))\n",
    "G_losses = [1]\n",
    "D_losses = [1]\n",
    "logger = Logger('GAN-RNN-SIN')\n",
    "for j, it in enumerate(e_bar):\n",
    "    G_loss_avg = 0\n",
    "    D_loss_avg = 0\n",
    "    n_batches = 100\n",
    "    b_bar = tqdm_notebook(range(n_batches), leave=False)\n",
    "    \n",
    "    DD = D_losses[-1]\n",
    "    GG = G_losses[-1]\n",
    "    \n",
    "    for i in b_bar:\n",
    "        z = Variable(g_noise()).cuda()\n",
    "        X, _ = real_sample()\n",
    "        X = Variable(X).cuda()\n",
    "        \n",
    "        G_sample = G(z)\n",
    "        D_real = D(X)\n",
    "        D_fake = D(G_sample)\n",
    "        D_loss_real = F.binary_cross_entropy_with_logits(D_real, ones_label)\n",
    "        D_loss_fake = F.binary_cross_entropy_with_logits(D_fake, zeros_label)     \n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        if DD / GG > 0.7: \n",
    "            D_solver.step()\n",
    "        D_loss = D_loss.data[0]\n",
    "        DD = D_loss / 2\n",
    "        reset_grad()\n",
    "        \n",
    "        for _ in range(1):\n",
    "            z = Variable(g_noise()).cuda()\n",
    "            G_sample = G(z)\n",
    "            D_fake = D(G_sample)\n",
    "            G_loss = F.binary_cross_entropy_with_logits(\n",
    "                D_fake, ones_label)\n",
    "            G_loss.backward()\n",
    "            if GG / DD > 0.7:\n",
    "                G_solver.step()\n",
    "            G_loss = G_loss.data[0]\n",
    "            GG = G_loss\n",
    "            reset_grad()\n",
    "        \n",
    "        D_loss_avg += D_loss\n",
    "        G_loss_avg += G_loss\n",
    "        \n",
    "        \n",
    "    G_losses.append(G_loss_avg / n_batches)\n",
    "    D_losses.append(D_loss_avg / n_batches)\n",
    "    G_loss_avg = 0\n",
    "    D_loss_avg = 0\n",
    "    \n",
    "    t = D_fake.mean().data.cpu().numpy().flatten()[0]\n",
    "    if j%10 == 0:\n",
    "        show_seqs(G_sample, j, t, ds=10)\n",
    "    logger.log_scalar('loss/Disciminator', D_losses[-1])\n",
    "    logger.log_scalar('loss/Generator', G_losses[-1])\n",
    "    e_bar.set_postfix(\n",
    "        G_loss=G_losses[-1],\n",
    "        D_loss=D_losses[-1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    z = Variable(g_noise()).cuda()\n",
    "    ex = 5\n",
    "    c = Variable(torch.ones(ex)).long().cuda() * 1\n",
    "    G_sample = G(z[0:ex,:], c)\n",
    "    ds = 1\n",
    "    show_seqs_nosave(z[0:ex], ds=ds)\n",
    "    show_seqs_nosave(G_sample[0:ex], ds=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    z = Variable(g_noise()).cuda()\n",
    "    c = Variable(torch.ones(5)).long().cuda() * 2\n",
    "    G_sample = G(z[0:5,:], c)\n",
    "    ds = 1\n",
    "    show_seqs_nosave(z, ds=ds)\n",
    "    show_seqs_aux(z[0:5],'input_noise', 'noise', ds=ds)\n",
    "    #show_seqs_aux(G_sample,'gen', 'condition: 2', ds=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
